{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# INSTALL ALL REQUIRED LIBS","metadata":{}},{"cell_type":"markdown","source":"# environment.yml","metadata":{}},{"cell_type":"code","source":"!pip install absl-py==0.12.0\n!pip install attrdict==2.0.1\n!pip install cachetools==4.2.1\n!pip install chardet==4.0.0\n!pip install cycler==0.10.0\n!pip install decorator==5.0.3\n!pip install google-auth==1.28.0\n!pip install google-auth-oauthlib==0.4.4\n!pip install grpcio==1.36.1\n!pip install idna==2.10\n!pip install imageio==2.9.0\n#!pip install kiwisolver==1.3.1\n!pip install markdown==3.3.4\n!pip install matplotlib==3.4.1\n!pip install networkx==2.5\n!pip install oauthlib==3.1.0\n!pip install opencv-python==4.5.1.48\n!pip install protobuf==3.15.6\n!pip install pyasn1==0.4.8\n!pip install pyasn1-modules==0.2.8\n!pip install pyparsing==2.4.7\n!pip install python-dateutil==2.8.1\n!pip install pywavelets==1.1.1\n!pip install requests==2.25.1\n!pip install requests-oauthlib==1.3.0\n!pip install rsa==4.7.2\n!pip install scikit-image==0.18.1\n!pip install scipy==1.6.2\n!pip install tensorboard==2.4.1\n!pip install tensorboard-plugin-wit==1.8.0\n!pip install tifffile==2021.3.31\n!pip install tqdm==4.59.0 urllib3==1.26.4 werkzeug==1.0.1\n\n!pip install certifi==2020.12.5\n!pip install ffmpeg==4.3\n!pip install intel-openmp=2020.0.2\n!pip install libtiff==4.1.0\n!pip install mkl==2020.2\n#!pip install mkl-service==2.3.0\n#!pip install mkl_fft==1.3.0\n#!pip install mkl_random==1.1.1\n!pip install ninja==1.10.2\n!pip install numpy==1.19.2\n!pip install olefile==0.46\n!pip install pillow==8.1.2\n!pip install zstd==1.4.9\n!pip install typing_extensions==3.7.4.3\n!pip install torchvision==0.9.1\n#!pip install torchaudio==0.8.1\n!pip install tk==8.6.10\n!pip install six==1.15.0\n!pip install setuptools==52.0.0\n!pip install readline==8.1\n\n#!pip install dataloader\n!pip install --upgrade torch torchvision","metadata":{"execution":{"iopub.status.busy":"2022-02-26T17:51:01.434430Z","iopub.execute_input":"2022-02-26T17:51:01.434759Z","iopub.status.idle":"2022-02-26T18:00:14.377777Z","shell.execute_reply.started":"2022-02-26T17:51:01.434671Z","shell.execute_reply":"2022-02-26T18:00:14.376968Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# DOWNLOAD TRAINING DATASET","metadata":{}},{"cell_type":"code","source":"#!cd /tmp\n#!mkdir ztemp\n#!cd /tmp/ztemp\n!pip install gdown\n#!gdown --id 121-KIDphLviZzz2ZNhnRZ4hz5KtjCYUh\n!gdown --id 1aTq_ZJtQNKJCTW0ykDg_6ANZkpgSZ_89\nimport zipfile\nwith zipfile.ZipFile('/tmp/ztemp/dataset4.zip', 'r') as zip_ref:\n    zip_ref.extractall('/tmp/ztemp/dataset')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:39:15.260737Z","iopub.execute_input":"2022-02-25T12:39:15.261011Z","iopub.status.idle":"2022-02-25T12:39:43.351999Z","shell.execute_reply.started":"2022-02-25T12:39:15.260963Z","shell.execute_reply":"2022-02-25T12:39:43.351171Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DOWNLOAD AND PREPARE TESTING IMAGE","metadata":{}},{"cell_type":"markdown","source":"# DATA FOLDER","metadata":{}},{"cell_type":"markdown","source":"# data/dataset.py","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os\nimport math\nimport numpy as np\nfrom glob import glob\n\nfrom random import shuffle\nfrom PIL import Image, ImageFilter\n\nimport torch\nimport torchvision.transforms.functional as TF\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\n\nclass InpaintingData(Dataset):\n    def __init__(self, args):\n        super(Dataset, self).__init__()\n        self.w = self.h = args.image_size\n        self.mask_type = args.mask_type\n        \n        # image and mask \n        self.image_path = []\n        for ext in ['*.jpg', '*.png']: \n            self.image_path.extend(glob(os.path.join(args.dir_image, args.data_train, ext)))\n        self.mask_path = glob(os.path.join(args.dir_mask, args.mask_type, '*.png'))\n\n        # augmentation \n        self.img_trans = transforms.Compose([\n            transforms.RandomResizedCrop(args.image_size),\n            transforms.RandomHorizontalFlip(),\n            transforms.ColorJitter(0.05, 0.05, 0.05, 0.05),\n            transforms.ToTensor()])\n        self.mask_trans = transforms.Compose([\n            transforms.Resize(args.image_size, interpolation=transforms.InterpolationMode.NEAREST),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(\n                (0, 45), interpolation=transforms.InterpolationMode.NEAREST),\n        ])\n\n        \n    def __len__(self):\n        return len(self.image_path)\n\n    def __getitem__(self, index):\n        # load image\n        image = Image.open(self.image_path[index]).convert('RGB')\n        filename = os.path.basename(self.image_path[index])\n\n        if self.mask_type == 'pconv':\n            index = np.random.randint(0, len(self.mask_path))\n            mask = Image.open(self.mask_path[index])\n            mask = mask.convert('L')\n        else:\n            mask = np.zeros((self.h, self.w)).astype(np.uint8)\n            mask[self.h//4:self.h//4*3, self.w//4:self.w//4*3] = 1\n            mask = Image.fromarray(m).convert('L')\n        \n        # augment\n        image = self.img_trans(image) * 2. - 1.\n        mask = TF.to_tensor(self.mask_trans(mask))\n\n        return image, mask, filename\n\n\n\n#if __name__ == '__main__': \n#\n#    from attrdict import AttrDict\n#    args = {\n#        'dir_image': '/tmp/ztemp/dataset/images',\n#        'data_train': 'iconrus',\n#        'dir_mask': '/tmp/ztemp/dataset/masks',\n#        'mask_type': 'pconv',\n#        'image_size': 512\n#    }\n#    args = AttrDict(args)\n#\n#    data = InpaintingData(args)\n#    print(len(data), len(data.mask_path))\n#    img, mask, filename = data[0]\n#    print(img.size(), mask.size(), filename)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:49:42.396459Z","iopub.execute_input":"2022-02-25T12:49:42.396757Z","iopub.status.idle":"2022-02-25T12:49:43.158296Z","shell.execute_reply.started":"2022-02-25T12:49:42.396725Z","shell.execute_reply":"2022-02-25T12:49:43.157533Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# data/common.py","metadata":{}},{"cell_type":"code","source":"\nimport zipfile\n\n\nclass ZipReader(object):\n    file_dict = dict()\n\n    def __init__(self):\n        super(ZipReader, self).__init__()\n\n    @staticmethod\n    def build_file_dict(path):\n        file_dict = ZipReader.file_dict\n        if path in file_dict:\n            return file_dict[path]\n        else:\n            file_handle = zipfile.ZipFile(path, mode='r', allowZip64=True)\n            file_dict[path] = file_handle\n            return file_dict[path]\n\n    @staticmethod\n    def imread(path, image_name):\n        zfile = ZipReader.build_file_dict(path)\n        data = zfile.read(image_name)\n        im = Image.open(io.BytesIO(data))\n        return im","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:49:46.886146Z","iopub.execute_input":"2022-02-25T12:49:46.886757Z","iopub.status.idle":"2022-02-25T12:49:46.902904Z","shell.execute_reply.started":"2022-02-25T12:49:46.886714Z","shell.execute_reply":"2022-02-25T12:49:46.901133Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# data/__init__.py","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n\ndef sample_data(loader): \n    while True:\n        for batch in loader:\n            yield batch\n\n\ndef create_loader(args): \n    dataset = InpaintingData(args)\n    data_loader = DataLoader(\n        dataset, batch_size=args.batch_size//args.world_size,\n        shuffle=True, num_workers=args.num_workers, pin_memory=True)\n    \n    return sample_data(data_loader)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:49:51.560902Z","iopub.execute_input":"2022-02-25T12:49:51.562124Z","iopub.status.idle":"2022-02-25T12:49:51.572244Z","shell.execute_reply.started":"2022-02-25T12:49:51.562083Z","shell.execute_reply":"2022-02-25T12:49:51.571621Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LOSS FOLDER","metadata":{}},{"cell_type":"markdown","source":"# loss/common.py","metadata":{}},{"cell_type":"code","source":"import torch \nimport torch.nn as nn \nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch.nn.functional import conv2d\n\n\nclass VGG19(nn.Module):\n    def __init__(self, resize_input=False):\n        super(VGG19, self).__init__()\n        features = models.vgg19(pretrained=True).features\n\n        self.resize_input = resize_input\n        self.mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n        self.std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n        prefix = [1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5]\n        posfix = [1, 2, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4]\n        names = list(zip(prefix, posfix))\n        self.relus = []\n        for pre, pos in names:\n            self.relus.append('relu{}_{}'.format(pre, pos))\n            self.__setattr__('relu{}_{}'.format(\n                pre, pos), torch.nn.Sequential())\n\n        nums = [[0, 1], [2, 3], [4, 5, 6], [7, 8],\n                [9, 10, 11], [12, 13], [14, 15], [16, 17],\n                [18, 19, 20], [21, 22], [23, 24], [25, 26],\n                [27, 28, 29], [30, 31], [32, 33], [34, 35]]\n\n        for i, layer in enumerate(self.relus):\n            for num in nums[i]:\n                self.__getattr__(layer).add_module(str(num), features[num])\n\n        # don't need the gradients, just want the features\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        # resize and normalize input for pretrained vgg19\n        x = (x + 1.0) / 2.0\n        x = (x - self.mean.view(1, 3, 1, 1)) / (self.std.view(1, 3, 1, 1))\n        if self.resize_input:\n            x = F.interpolate(\n                x, size=(256, 256), mode='bilinear', align_corners=True)\n        features = []\n        for layer in self.relus:\n            x = self.__getattr__(layer)(x)\n            features.append(x)\n        out = {key: value for (key, value) in list(zip(self.relus, features))}\n        return out\n\n\ndef gaussian(window_size, sigma):\n    def gauss_fcn(x):\n        return -(x - window_size // 2)**2 / float(2 * sigma**2)\n    gauss = torch.stack([torch.exp(torch.tensor(gauss_fcn(x)))\n                         for x in range(window_size)])\n    return gauss / gauss.sum()\n\n\ndef get_gaussian_kernel(kernel_size: int, sigma: float) -> torch.Tensor:\n    r\"\"\"Function that returns Gaussian filter coefficients.\n    Args:\n      kernel_size (int): filter size. It should be odd and positive.\n      sigma (float): gaussian standard deviation.\n    Returns:\n      Tensor: 1D tensor with gaussian filter coefficients.\n    Shape:\n      - Output: :math:`(\\text{kernel_size})`\n\n    Examples::\n      >>> kornia.image.get_gaussian_kernel(3, 2.5)\n      tensor([0.3243, 0.3513, 0.3243])\n      >>> kornia.image.get_gaussian_kernel(5, 1.5)\n      tensor([0.1201, 0.2339, 0.2921, 0.2339, 0.1201])\n    \"\"\"\n    if not isinstance(kernel_size, int) or kernel_size % 2 == 0 or kernel_size <= 0:\n        raise TypeError(\n            \"kernel_size must be an odd positive integer. Got {}\".format(kernel_size))\n    window_1d: torch.Tensor = gaussian(kernel_size, sigma)\n    return window_1d\n\n\ndef get_gaussian_kernel2d(kernel_size, sigma):\n    r\"\"\"Function that returns Gaussian filter matrix coefficients.\n    Args:\n      kernel_size (Tuple[int, int]): filter sizes in the x and y direction.\n        Sizes should be odd and positive.\n      sigma (Tuple[int, int]): gaussian standard deviation in the x and y\n        direction.\n    Returns:\n      Tensor: 2D tensor with gaussian filter matrix coefficients.\n\n    Shape:\n      - Output: :math:`(\\text{kernel_size}_x, \\text{kernel_size}_y)`\n\n    Examples::\n      >>> kornia.image.get_gaussian_kernel2d((3, 3), (1.5, 1.5))\n      tensor([[0.0947, 0.1183, 0.0947],\n              [0.1183, 0.1478, 0.1183],\n              [0.0947, 0.1183, 0.0947]])\n\n      >>> kornia.image.get_gaussian_kernel2d((3, 5), (1.5, 1.5))\n      tensor([[0.0370, 0.0720, 0.0899, 0.0720, 0.0370],\n              [0.0462, 0.0899, 0.1123, 0.0899, 0.0462],\n              [0.0370, 0.0720, 0.0899, 0.0720, 0.0370]])\n    \"\"\"\n    if not isinstance(kernel_size, tuple) or len(kernel_size) != 2:\n        raise TypeError(\n            \"kernel_size must be a tuple of length two. Got {}\".format(kernel_size))\n    if not isinstance(sigma, tuple) or len(sigma) != 2:\n        raise TypeError(\n            \"sigma must be a tuple of length two. Got {}\".format(sigma))\n    ksize_x, ksize_y = kernel_size\n    sigma_x, sigma_y = sigma\n    kernel_x: torch.Tensor = get_gaussian_kernel(ksize_x, sigma_x)\n    kernel_y: torch.Tensor = get_gaussian_kernel(ksize_y, sigma_y)\n    kernel_2d: torch.Tensor = torch.matmul(\n        kernel_x.unsqueeze(-1), kernel_y.unsqueeze(-1).t())\n    return kernel_2d\n\n\nclass GaussianBlur(nn.Module):\n    r\"\"\"Creates an operator that blurs a tensor using a Gaussian filter.\n    The operator smooths the given tensor with a gaussian kernel by convolving\n    it to each channel. It suports batched operation.\n    Arguments:\n      kernel_size (Tuple[int, int]): the size of the kernel.\n      sigma (Tuple[float, float]): the standard deviation of the kernel.\n    Returns:\n      Tensor: the blurred tensor.\n    Shape:\n      - Input: :math:`(B, C, H, W)`\n      - Output: :math:`(B, C, H, W)`\n\n    Examples::\n      >>> input = torch.rand(2, 4, 5, 5)\n      >>> gauss = kornia.filters.GaussianBlur((3, 3), (1.5, 1.5))\n      >>> output = gauss(input)  # 2x4x5x5\n    \"\"\"\n\n    def __init__(self, kernel_size, sigma):\n        super(GaussianBlur, self).__init__()\n        self.kernel_size = kernel_size\n        self.sigma = sigma\n        self._padding = self.compute_zero_padding(kernel_size)\n        self.kernel = get_gaussian_kernel2d(kernel_size, sigma)\n\n    @staticmethod\n    def compute_zero_padding(kernel_size):\n        \"\"\"Computes zero padding tuple.\"\"\"\n        computed = [(k - 1) // 2 for k in kernel_size]\n        return computed[0], computed[1]\n\n    def forward(self, x):  # type: ignore\n        if not torch.is_tensor(x):\n            raise TypeError(\n                \"Input x type is not a torch.Tensor. Got {}\".format(type(x)))\n        if not len(x.shape) == 4:\n            raise ValueError(\n                \"Invalid input shape, we expect BxCxHxW. Got: {}\".format(x.shape))\n        # prepare kernel\n        b, c, h, w = x.shape\n        tmp_kernel: torch.Tensor = self.kernel.to(x.device).to(x.dtype)\n        kernel: torch.Tensor = tmp_kernel.repeat(c, 1, 1, 1)\n\n        # TODO: explore solution when using jit.trace since it raises a warning\n        # because the shape is converted to a tensor instead to a int.\n        # convolve tensor with gaussian kernel\n        return conv2d(x, kernel, padding=self._padding, stride=1, groups=c)\n\n\n######################\n# functional interface\n######################\n\ndef gaussian_blur(input, kernel_size, sigma):\n    r\"\"\"Function that blurs a tensor using a Gaussian filter.\n    See :class:`~kornia.filters.GaussianBlur` for details.\n    \"\"\"\n    return GaussianBlur(kernel_size, sigma)(input)\n\n\n#if __name__ == '__main__':\n#    img = Image.open('test.png').convert('L')\n#    tensor_img = F.to_tensor(img).unsqueeze(0).float()\n#    print('tensor_img size: ', tensor_img.size())\n#\n#    blurred_img = gaussian_blur(tensor_img, (61, 61), (10, 10))\n#    print(torch.min(blurred_img), torch.max(blurred_img))\n#\n#    blurred_img = blurred_img*255\n#    img = blurred_img.int().numpy().astype(np.uint8)[0][0]\n#    print(img.shape, np.min(img), np.max(img), np.unique(img))\n#    cv2.imwrite('gaussian.png', img)\n#","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:49:55.130427Z","iopub.execute_input":"2022-02-25T12:49:55.130845Z","iopub.status.idle":"2022-02-25T12:49:55.164579Z","shell.execute_reply.started":"2022-02-25T12:49:55.130809Z","shell.execute_reply":"2022-02-25T12:49:55.16388Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# loss/loss.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass L1(): \n    def __init__(self,):\n        self.calc = torch.nn.L1Loss()\n    \n    def __call__(self, x, y):\n        return self.calc(x, y)\n\n\nclass Perceptual(nn.Module):\n    def __init__(self, weights=[1.0, 1.0, 1.0, 1.0, 1.0]):\n        super(Perceptual, self).__init__()\n        self.vgg = VGG19().cuda()\n        self.criterion = torch.nn.L1Loss()\n        self.weights = weights\n\n    def __call__(self, x, y):\n        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n        content_loss = 0.0\n        prefix = [1, 2, 3, 4, 5]\n        for i in range(5):\n            content_loss += self.weights[i] * self.criterion(\n                x_vgg[f'relu{prefix[i]}_1'], y_vgg[f'relu{prefix[i]}_1'])\n        return content_loss\n\n\nclass Style(nn.Module):\n    def __init__(self):\n        super(Style, self).__init__()\n        self.vgg = VGG19().cuda()\n        self.criterion = torch.nn.L1Loss()\n\n    def compute_gram(self, x):\n        b, c, h, w = x.size()\n        f = x.view(b, c, w * h)\n        f_T = f.transpose(1, 2)\n        G = f.bmm(f_T) / (h * w * c)\n        return G\n\n    def __call__(self, x, y):\n        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n        style_loss = 0.0\n        prefix = [2, 3, 4, 5]\n        posfix = [2, 4, 4, 2]\n        for pre, pos in list(zip(prefix, posfix)):\n            style_loss += self.criterion(\n                self.compute_gram(x_vgg[f'relu{pre}_{pos}']), self.compute_gram(y_vgg[f'relu{pre}_{pos}']))\n        return style_loss\n\n\nclass nsgan(): \n    def __init__(self, ):\n        self.loss_fn = torch.nn.Softplus()\n    \n    def __call__(self, netD, fake, real):\n        fake_detach = fake.detach()\n        d_fake = netD(fake_detach)\n        d_real = netD(real)\n        dis_loss = self.loss_fn(-d_real).mean() + self.loss_fn(d_fake).mean()\n\n        g_fake = netD(fake)\n        gen_loss = self.loss_fn(-g_fake).mean()\n        \n        return dis_loss, gen_loss\n\n\nclass smgan():\n    def __init__(self, ksize=71): \n        self.ksize = ksize\n        self.loss_fn = nn.MSELoss()\n    \n    def __call__(self, netD, fake, real, masks): \n        fake_detach = fake.detach()\n\n        g_fake = netD(fake)\n        d_fake  = netD(fake_detach)\n        d_real = netD(real)\n\n        _, _, h, w = g_fake.size()\n        b, c, ht, wt = masks.size()\n        \n        # Handle inconsistent size between outputs and masks\n        if h != ht or w != wt:\n            g_fake = F.interpolate(g_fake, size=(ht, wt), mode='bilinear', align_corners=True)\n            d_fake = F.interpolate(d_fake, size=(ht, wt), mode='bilinear', align_corners=True)\n            d_real = F.interpolate(d_real, size=(ht, wt), mode='bilinear', align_corners=True)\n        d_fake_label = gaussian_blur(masks, (self.ksize, self.ksize), (10, 10)).detach().cuda()\n        d_real_label = torch.zeros_like(d_real).cuda()\n        g_fake_label = torch.ones_like(g_fake).cuda()\n\n        dis_loss = self.loss_fn(d_fake, d_fake_label) + self.loss_fn(d_real, d_real_label)\n        gen_loss = self.loss_fn(g_fake, g_fake_label) * masks / torch.mean(masks)\n\n        return dis_loss.mean(), gen_loss.mean()\n        \n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:50:00.436089Z","iopub.execute_input":"2022-02-25T12:50:00.436335Z","iopub.status.idle":"2022-02-25T12:50:00.459633Z","shell.execute_reply.started":"2022-02-25T12:50:00.436308Z","shell.execute_reply":"2022-02-25T12:50:00.458881Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# METRIC FOLDER","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# metric/inception.py","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\nclass InceptionV3(nn.Module):\n    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n    # Index of default block of inception to return,\n    # corresponds to output of final average pooling\n    DEFAULT_BLOCK_INDEX = 3\n\n    # Maps feature dimensionality to their output blocks indices\n    BLOCK_INDEX_BY_DIM = {\n        64: 0,   # First max pooling features\n        192: 1,  # Second max pooling featurs\n        768: 2,  # Pre-aux classifier features\n        2048: 3  # Final average pooling features\n    }\n\n    def __init__(self, output_blocks=[DEFAULT_BLOCK_INDEX],\n                 resize_input=True, normalize_input=True, requires_grad=False):\n        \"\"\"Build pretrained InceptionV3\n        Parameters\n        ----------\n        output_blocks : list of int\n          Indices of blocks to return features of. Possible values are:\n            - 0: corresponds to output of first max pooling\n            - 1: corresponds to output of second max pooling\n            - 2: corresponds to output which is fed to aux classifier\n            - 3: corresponds to output of final average pooling\n        resize_input : bool\n          If true, bilinearly resizes input to width and height 299 before\n          feeding input to model. As the network without fully connected\n          layers is fully convolutional, it should be able to handle inputs\n          of arbitrary size, so resizing might not be strictly needed\n        normalize_input : bool\n          If true, normalizes the input to the statistics the pretrained\n          Inception network expects\n        requires_grad : bool\n          If true, parameters of the model require gradient. Possibly useful\n          for finetuning the network\n        \"\"\"\n        super(InceptionV3, self).__init__()\n        self.resize_input = resize_input\n        self.normalize_input = normalize_input\n        self.output_blocks = sorted(output_blocks)\n        self.last_needed_block = max(output_blocks)\n\n        assert self.last_needed_block <= 3, \\\n            'Last possible output block index is 3'\n\n        self.blocks = nn.ModuleList()\n        inception = models.inception_v3(pretrained=True)\n\n        # Block 0: input to maxpool1\n        block0 = [\n            inception.Conv2d_1a_3x3,\n            inception.Conv2d_2a_3x3,\n            inception.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        ]\n        self.blocks.append(nn.Sequential(*block0))\n\n        # Block 1: maxpool1 to maxpool2\n        if self.last_needed_block >= 1:\n            block1 = [\n                inception.Conv2d_3b_1x1,\n                inception.Conv2d_4a_3x3,\n                nn.MaxPool2d(kernel_size=3, stride=2)\n            ]\n            self.blocks.append(nn.Sequential(*block1))\n\n        # Block 2: maxpool2 to aux classifier\n        if self.last_needed_block >= 2:\n            block2 = [\n                inception.Mixed_5b,\n                inception.Mixed_5c,\n                inception.Mixed_5d,\n                inception.Mixed_6a,\n                inception.Mixed_6b,\n                inception.Mixed_6c,\n                inception.Mixed_6d,\n                inception.Mixed_6e,\n            ]\n            self.blocks.append(nn.Sequential(*block2))\n\n        # Block 3: aux classifier to final avgpool\n        if self.last_needed_block >= 3:\n            block3 = [\n                inception.Mixed_7a,\n                inception.Mixed_7b,\n                inception.Mixed_7c,\n                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n            ]\n            self.blocks.append(nn.Sequential(*block3))\n\n        for param in self.parameters():\n            param.requires_grad = requires_grad\n\n    def forward(self, inp):\n        \"\"\"Get Inception feature maps\n        Parameters\n        ----------\n        inp : torch.autograd.Variable\n          Input tensor of shape Bx3xHxW. Values are expected to be in \n          range (0, 1)\n        Returns\n        -------\n        List of torch.autograd.Variable, corresponding to the selected output \n        block, sorted ascending by index\n        \"\"\"\n        outp = []\n        x = inp\n        if self.resize_input:\n            x = F.interpolate(x, size=(299, 299),\n                              mode='bilinear', align_corners=True)\n\n        if self.normalize_input:\n            x = x.clone()\n            x[:, 0] = x[:, 0] * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n            x[:, 1] = x[:, 1] * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n            x[:, 2] = x[:, 2] * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n\n        for idx, block in enumerate(self.blocks):\n            x = block(x)\n            if idx in self.output_blocks:\n                outp.append(x)\n            if idx == self.last_needed_block:\n                break\n        return outp\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:50:04.76535Z","iopub.execute_input":"2022-02-25T12:50:04.765601Z","iopub.status.idle":"2022-02-25T12:50:04.783493Z","shell.execute_reply.started":"2022-02-25T12:50:04.765574Z","shell.execute_reply":"2022-02-25T12:50:04.782817Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# metric/metric.py","metadata":{}},{"cell_type":"code","source":"import os \nimport pickle\nimport numpy as np\nfrom tqdm import tqdm\nfrom scipy import linalg\nfrom multiprocessing import Pool\nfrom skimage.metrics import structural_similarity\nfrom skimage.metrics import peak_signal_noise_ratio\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch.nn.functional import adaptive_avg_pool2d\n\n# ============================\n\ndef compare_mae(pairs):\n    real, fake = pairs\n    real, fake = real.astype(np.float32), fake.astype(np.float32)\n    return np.sum(np.abs(real - fake)) / np.sum(real + fake)\n\ndef compare_psnr(pairs):\n    real, fake = pairs\n    return peak_signal_noise_ratio(real, fake)\n\ndef compare_ssim(pairs):\n    real, fake = pairs\n    return structural_similarity(real, fake, multichannel=True)\n\n# ================================\n\ndef mae(reals, fakes, num_worker=8):\n    error = 0\n    pool = Pool(num_worker)\n    for val in tqdm(pool.imap_unordered(compare_mae, zip(reals, fakes)), total=len(reals), desc='compare_mae'):\n        error += val \n    return error / len(reals)\n\ndef psnr(reals, fakes, num_worker=8):\n    error = 0\n    pool = Pool(num_worker)\n    for val in tqdm(pool.imap_unordered(compare_psnr, zip(reals, fakes)), total=len(reals), desc='compare_psnr'):\n        error += val\n    return error / len(reals)\n\ndef ssim(reals, fakes, num_worker=8):\n    error = 0\n    pool = Pool(num_worker)\n    for val in tqdm(pool.imap_unordered(compare_ssim, zip(reals, fakes)), total=len(reals), desc='compare_ssim'):\n        error += val\n    return error / len(reals)\n\ndef fid(reals, fakes, num_worker=8, real_fid_path=None):\n    \n    dims = 2048\n    batch_size = 4\n    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n    model = InceptionV3([block_idx]).cuda()\n\n    if real_fid_path is None: \n        real_fid_path = 'places2_fid.pt'\n        \n    if os.path.isfile(real_fid_path): \n        data = pickle.load(open(real_fid_path, 'rb'))\n        real_m, real_s = data['mu'], data['sigma']\n    else: \n        reals = (np.array(reals).astype(np.float32) / 255.0).transpose((0, 3, 1, 2))\n        real_m, real_s = calculate_activation_statistics(reals, model, batch_size, dims)\n        with open(real_fid_path, 'wb') as f: \n            pickle.dump({'mu': real_m, 'sigma': real_s}, f)\n\n\n    # calculate fid statistics for fake images\n    fakes = (np.array(fakes).astype(np.float32) / 255.0).transpose((0, 3, 1, 2))\n    fake_m, fake_s = calculate_activation_statistics(fakes, model, batch_size, dims)\n\n    fid_value = calculate_frechet_distance(real_m, real_s, fake_m, fake_s)\n\n    return fid_value\n\n\ndef calculate_activation_statistics(images, model, batch_size=64,\n                                    dims=2048, cuda=True, verbose=False):\n    \"\"\"Calculation of the statistics used by the FID.\n    Params:\n    -- images      : Numpy array of dimension (n_images, 3, hi, wi). The values\n                     must lie between 0 and 1.\n    -- model       : Instance of inception model\n    -- batch_size  : The images numpy array is split into batches with\n                     batch size batch_size. A reasonable batch size\n                     depends on the hardware.\n    -- dims        : Dimensionality of features returned by Inception\n    -- cuda        : If set to True, use GPU\n    -- verbose     : If set to True and parameter out_step is given, the\n                     number of calculated batches is reported.\n    Returns:\n    -- mu    : The mean over samples of the activations of the pool_3 layer of\n               the inception model.\n    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n               the inception model.\n    \"\"\"\n    act = get_activations(images, model, batch_size, dims, cuda, verbose)\n    mu = np.mean(act, axis=0)\n    sigma = np.cov(act, rowvar=False)\n    return mu, sigma\n\n\ndef get_activations(images, model, batch_size=64, dims=2048, cuda=True, verbose=False):\n    \"\"\"Calculates the activations of the pool_3 layer for all images.\n    Params:\n    -- images      : Numpy array of dimension (n_images, 3, hi, wi). The values\n                     must lie between 0 and 1.\n    -- model       : Instance of inception model\n    -- batch_size  : the images numpy array is split into batches with\n                     batch size batch_size. A reasonable batch size depends\n                     on the hardware.\n    -- dims        : Dimensionality of features returned by Inception\n    -- cuda        : If set to True, use GPU\n    -- verbose     : If set to True and parameter out_step is given, the number\n                     of calculated batches is reported.\n    Returns:\n    -- A numpy array of dimension (num images, dims) that contains the\n       activations of the given tensor when feeding inception with the\n       query tensor.\n    \"\"\"\n    model.eval()\n\n    d0 = images.shape[0]\n    if batch_size > d0:\n        print(('Warning: batch size is bigger than the data size. '\n               'Setting batch size to data size'))\n        batch_size = d0\n\n    n_batches = d0 // batch_size\n    n_used_imgs = n_batches * batch_size\n\n    pred_arr = np.empty((n_used_imgs, dims))\n    for i in tqdm(range(n_batches), desc='calculate activations'):\n        if verbose:\n            print('\\rPropagating batch %d/%d' %\n                  (i + 1, n_batches), end='', flush=True)\n        start = i * batch_size\n        end = start + batch_size\n\n        batch = torch.from_numpy(images[start:end]).type(torch.FloatTensor)\n        batch = Variable(batch)\n        if torch.cuda.is_available:\n            batch = batch.cuda()\n        with torch.no_grad():\n            pred = model(batch)[0]\n\n        # If model output is not scalar, apply global spatial average pooling.\n        # This happens if you choose a dimensionality not equal 2048.\n        if pred.shape[2] != 1 or pred.shape[3] != 1:\n            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n        pred_arr[start:end] = pred.cpu().data.numpy().reshape(batch_size, -1)\n    if verbose:\n        print(' done')\n\n    return pred_arr\n\n\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n    Stable version by Dougal J. Sutherland.\n    Params:\n    -- mu1   : Numpy array containing the activations of a layer of the\n               inception net (like returned by the function 'get_predictions')\n               for generated samples.\n    -- mu2   : The sample mean over activations, precalculated on an \n               representive data set.\n    -- sigma1: The covariance matrix over activations for generated samples.\n    -- sigma2: The covariance matrix over activations, precalculated on an \n               representive data set.\n    Returns:\n    --   : The Frechet Distance.\n    \"\"\"\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    assert mu1.shape == mu2.shape, 'Training and test mean vectors have different lengths'\n    assert sigma1.shape == sigma2.shape, 'Training and test covariances have different dimensions'\n    diff = mu1 - mu2\n\n    # Product might be almost singular\n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    if not np.isfinite(covmean).all():\n        msg = ('fid calculation produces singular product; '\n               'adding %s to diagonal of cov estimates') % eps\n        print(msg)\n        offset = np.eye(sigma1.shape[0]) * eps\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n    # Numerical error might give slight imaginary component\n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError('Imaginary component {}'.format(m))\n        covmean = covmean.real\n    tr_covmean = np.trace(covmean)\n\n    return (diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:50:08.070893Z","iopub.execute_input":"2022-02-25T12:50:08.071434Z","iopub.status.idle":"2022-02-25T12:50:08.709441Z","shell.execute_reply.started":"2022-02-25T12:50:08.071399Z","shell.execute_reply":"2022-02-25T12:50:08.708713Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL FOLDER","metadata":{}},{"cell_type":"markdown","source":"# model/common.py","metadata":{}},{"cell_type":"code","source":"\nimport torch \nimport torch.nn as nn \n\n\nclass BaseNetwork(nn.Module):\n    module=None\n    def __init__(self):\n        super(BaseNetwork, self).__init__()\n        self.module=super(BaseNetwork,self)\n    def print_network(self):\n        if isinstance(self, list):\n            self = self[0]\n        num_params = 0\n        for param in self.parameters():\n            num_params += param.numel()\n        print('Network [%s] was created. Total number of parameters: %.1f million. '\n              'To see the architecture, do print(network).' % (type(self).__name__, num_params / 1000000))\n\n    def init_weights(self, init_type='normal', gain=0.02):\n        '''\n        initialize network's weights\n        init_type: normal | xavier | kaiming | orthogonal\n        https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/9451e70673400885567d08a9e97ade2524c700d0/models/networks.py#L39\n        '''\n        def init_func(m):\n            classname = m.__class__.__name__\n            if classname.find('InstanceNorm2d') != -1:\n                if hasattr(m, 'weight') and m.weight is not None:\n                    nn.init.constant_(m.weight.data, 1.0)\n                if hasattr(m, 'bias') and m.bias is not None:\n                    nn.init.constant_(m.bias.data, 0.0)\n            elif hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n                if init_type == 'normal':\n                    nn.init.normal_(m.weight.data, 0.0, gain)\n                elif init_type == 'xavier':\n                    nn.init.xavier_normal_(m.weight.data, gain=gain)\n                elif init_type == 'xavier_uniform':\n                    nn.init.xavier_uniform_(m.weight.data, gain=1.0)\n                elif init_type == 'kaiming':\n                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n                elif init_type == 'orthogonal':\n                    nn.init.orthogonal_(m.weight.data, gain=gain)\n                elif init_type == 'none':  # uses pytorch's default init method\n                    m.reset_parameters()\n                else:\n                    raise NotImplementedError(\n                        'initialization method [%s] is not implemented' % init_type)\n                if hasattr(m, 'bias') and m.bias is not None:\n                    nn.init.constant_(m.bias.data, 0.0)\n\n        self.apply(init_func)\n\n        # propagate to children\n        for m in self.children():\n            if hasattr(m, 'init_weights'):\n                m.init_weights(init_type, gain)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:50:12.33087Z","iopub.execute_input":"2022-02-25T12:50:12.331435Z","iopub.status.idle":"2022-02-25T12:50:12.346514Z","shell.execute_reply.started":"2022-02-25T12:50:12.331395Z","shell.execute_reply":"2022-02-25T12:50:12.345848Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model/aotgan.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils import spectral_norm\n\nclass InpaintGenerator(BaseNetwork):\n    def __init__(self, args):  # 1046\n        super(InpaintGenerator, self).__init__()\n        module=super(InpaintGenerator, self).module\n        self.encoder = nn.Sequential(\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(4, 64, 7),\n            nn.ReLU(True),\n            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(128, 256, 4, stride=2, padding=1),\n            nn.ReLU(True)\n        )\n\n        self.middle = nn.Sequential(*[AOTBlock(256, args.rates) for _ in range(args.block_num)])\n\n        self.decoder = nn.Sequential(\n            UpConv(256, 128),\n            nn.ReLU(True),\n            UpConv(128, 64),\n            nn.ReLU(True),\n            nn.Conv2d(64, 3, 3, stride=1, padding=1)\n        )\n\n        self.init_weights()\n\n    def forward(self, x, mask):\n        x = torch.cat([x, mask], dim=1)\n        x = self.encoder(x)\n        x = self.middle(x)\n        x = self.decoder(x)\n        x = torch.tanh(x)\n        return x\n\n\nclass UpConv(nn.Module):\n    def __init__(self, inc, outc, scale=2):\n        super(UpConv, self).__init__()\n        self.scale = scale\n        self.conv = nn.Conv2d(inc, outc, 3, stride=1, padding=1)\n\n    def forward(self, x):\n        return self.conv(F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True))\n\n\nclass AOTBlock(nn.Module):\n    def __init__(self, dim, rates):\n        super(AOTBlock, self).__init__()\n        self.rates = rates\n        for i, rate in enumerate(rates):\n            self.__setattr__(\n                'block{}'.format(str(i).zfill(2)), \n                nn.Sequential(\n                    nn.ReflectionPad2d(rate),\n                    nn.Conv2d(dim, dim//4, 3, padding=0, dilation=rate),\n                    nn.ReLU(True)))\n        self.fuse = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(dim, dim, 3, padding=0, dilation=1))\n        self.gate = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(dim, dim, 3, padding=0, dilation=1))\n\n    def forward(self, x):\n        out = [self.__getattr__(f'block{str(i).zfill(2)}')(x) for i in range(len(self.rates))]\n        out = torch.cat(out, 1)\n        out = self.fuse(out)\n        mask = my_layer_norm(self.gate(x))\n        mask = torch.sigmoid(mask)\n        return x * (1 - mask) + out * mask\n\n\ndef my_layer_norm(feat):\n    mean = feat.mean((2, 3), keepdim=True)\n    std = feat.std((2, 3), keepdim=True) + 1e-9\n    feat = 2 * (feat - mean) / std - 1\n    feat = 5 * feat\n    return feat\n\n\n\n\n# ----- discriminator -----\nclass Discriminator(BaseNetwork):\n    def __init__(self, ):\n        super(Discriminator, self).__init__()\n        inc = 3\n        self.conv = nn.Sequential(\n            spectral_norm(nn.Conv2d(inc, 64, 4, stride=2, padding=1, bias=False)),\n            nn.LeakyReLU(0.2, inplace=True),\n            spectral_norm(nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False)),\n            nn.LeakyReLU(0.2, inplace=True),\n            spectral_norm(nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=False)),\n            nn.LeakyReLU(0.2, inplace=True),\n            spectral_norm(nn.Conv2d(256, 512, 4, stride=1, padding=1, bias=False)),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, 4, stride=1, padding=1)\n        )\n\n        self.init_weights()\n\n    def forward(self, x):\n        feat = self.conv(x)\n        return feat\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:50:19.032336Z","iopub.execute_input":"2022-02-25T12:50:19.032612Z","iopub.status.idle":"2022-02-25T12:50:19.061136Z","shell.execute_reply.started":"2022-02-25T12:50:19.032581Z","shell.execute_reply":"2022-02-25T12:50:19.060268Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINER FOLDER","metadata":{}},{"cell_type":"markdown","source":"# trainer/common.py","metadata":{}},{"cell_type":"code","source":"import time\nimport numpy as np\n\nimport torch\nfrom torch import distributed as dist\n\n\nclass timer():\n    def __init__(self):\n        self.acc = 0\n        self.t0 = torch.cuda.Event(enable_timing=True)\n        self.t1 = torch.cuda.Event(enable_timing=True)\n        self.tic()\n\n    def tic(self):\n        self.t0.record()\n\n    def toc(self, restart=False):\n        self.t1.record()\n        torch.cuda.synchronize()\n        diff = self.t0.elapsed_time(self.t1) /1000.\n        if restart: self.tic()\n        return diff\n\n    def hold(self):\n        self.acc += self.toc()\n\n    def release(self):\n        ret = self.acc\n        self.acc = 0\n\n        return ret\n\n    def reset(self):\n        self.acc = 0\n\n\ndef reduce_loss_dict(loss_dict, world_size):\n    if world_size == 1:\n        return loss_dict\n\n    with torch.no_grad():\n        keys = []\n        losses = []\n\n        for k in sorted(loss_dict.keys()):\n            keys.append(k)\n            losses.append(loss_dict[k])\n\n        losses = torch.stack(losses, 0)\n        dist.reduce(losses, dst=0)\n\n        if dist.get_rank() == 0:\n            losses /= world_size\n\n        reduced_losses = {k: v for k, v in zip(keys, losses)}\n    return reduced_losses\n            ","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:50:22.378713Z","iopub.execute_input":"2022-02-25T12:50:22.379278Z","iopub.status.idle":"2022-02-25T12:50:22.390491Z","shell.execute_reply.started":"2022-02-25T12:50:22.379238Z","shell.execute_reply":"2022-02-25T12:50:22.389819Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# trainer/trainer.py","metadata":{}},{"cell_type":"code","source":"import os\nimport importlib\nfrom tqdm import tqdm\nfrom glob import glob\n\nimport torch\nimport torch.optim as optim\nfrom torchvision.utils import make_grid\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nclass Trainer():\n    def __init__(self, args):\n        self.args = args \n        self.iteration = 0\n\n        # setup data set and data loader\n        self.dataloader = create_loader(args)\n\n        # set up losses and metrics\n        #self.rec_loss_func = {\n        #    key: getattr(loss, key)() for key, val in args.rec_loss.items()}\n        self.rec_loss_func = {\n            'L1':L1(),\n            'Style':Style(),\n            'Perceptual':Perceptual()\n        }\n        self.adv_loss = smgan()\n        #self.adv_loss = getattr(loss, args.gan_type)()\n\n        # Image generator input: [rgb(3) + mask(1)], discriminator input: [rgb(3)]\n        #net = importlib.import_module('model.'+args.model)\n        \n        self.netG = InpaintGenerator(args).cuda()\n        self.optimG = torch.optim.Adam(\n            self.netG.parameters(), lr=args.lrg, betas=(args.beta1, args.beta2))\n\n        self.netD = Discriminator().cuda()\n        self.optimD = torch.optim.Adam(\n            self.netD.parameters(), lr=args.lrd, betas=(args.beta1, args.beta2))\n        \n        self.load()\n        if args.distributed:\n            self.netG = DDP(self.netG, device_ids= [args.local_rank], output_device=[args.local_rank])\n            self.netD = DDP(self.netD, device_ids= [args.local_rank], output_device=[args.local_rank])\n        \n        if args.tensorboard: \n            self.writer = SummaryWriter(os.path.join(args.save_dir, 'log'))\n            \n\n    def load(self):\n        try: \n            gpath = sorted(list(glob(os.path.join(self.args.save_dir, 'G*.pt'))))[-1]\n            self.netG.load_state_dict(torch.load(gpath, map_location='cuda'))\n            self.iteration = int(os.path.basename(gpath)[1:-3])\n            if self.args.global_rank == 0: \n                print(f'[**] Loading generator network from {gpath}')\n        except: \n            pass \n        \n        try: \n            dpath = sorted(list(glob(os.path.join(self.args.save_dir, 'D*.pt'))))[-1]\n            self.netD.load_state_dict(torch.load(dpath, map_location='cuda'))\n            if self.args.global_rank == 0: \n                print(f'[**] Loading discriminator network from {dpath}')\n        except: \n            pass\n        \n        try: \n            opath = sorted(list(glob(os.path.join(self.args.save_dir, 'O*.pt'))))[-1]\n            data = torch.load(opath, map_location='cuda')\n            self.optimG.load_state_dict(data['optimG'])\n            self.optimD.load_state_dict(data['optimD'])\n            if self.args.global_rank == 0: \n                print(f'[**] Loading optimizer from {opath}')\n        except: \n            pass\n\n\n    def save(self, ):\n        if self.args.global_rank == 0:\n            print(f'\\nsaving {self.iteration} model to {self.args.save_dir} ...')\n            torch.save(self.netG.module.state_dict(), \n                os.path.join(self.args.save_dir, f'G{str(self.iteration).zfill(7)}.pt'))\n            torch.save(self.netD.module.state_dict(), \n                os.path.join(self.args.save_dir, f'D{str(self.iteration).zfill(7)}.pt'))\n            torch.save(\n                {'optimG': self.optimG.state_dict(), 'optimD': self.optimD.state_dict()}, \n                os.path.join(self.args.save_dir, f'O{str(self.iteration).zfill(7)}.pt'))\n            \n\n    def train(self):\n        pbar = range(self.iteration, self.args.iterations)\n        if self.args.global_rank == 0: \n            pbar = tqdm(range(self.args.iterations), initial=self.iteration, dynamic_ncols=True, smoothing=0.01)\n            timer_data, timer_model = timer(), timer()\n        \n        for idx in pbar:\n            self.iteration += 1\n            images, masks, filename = next(self.dataloader)\n            images, masks = images.cuda(), masks.cuda()\n            images_masked = (images * (1 - masks).float()) + masks\n\n            if self.args.global_rank == 0: \n                timer_data.hold()\n                timer_model.tic()\n\n            # in: [rgb(3) + edge(1)]\n            pred_img = self.netG(images_masked, masks)\n            comp_img = (1 - masks) * images + masks * pred_img\n\n            # reconstruction losses \n            losses = {}\n            for name, weight in self.args.rec_loss.items(): \n                losses[name] = weight * self.rec_loss_func[name](pred_img, images)\n            \n            # adversarial loss \n            dis_loss, gen_loss = self.adv_loss(self.netD, comp_img, images, masks)\n            losses[f\"advg\"] = gen_loss * self.args.adv_weight\n            \n            # backforward \n            self.optimG.zero_grad()\n            self.optimD.zero_grad()\n            sum(losses.values()).backward()\n            losses[f\"advd\"] = dis_loss \n            dis_loss.backward()\n            self.optimG.step()\n            self.optimD.step()\n\n            if self.args.global_rank == 0:\n                timer_model.hold()\n                timer_data.tic()\n\n            # logs\n            scalar_reduced = reduce_loss_dict(losses, self.args.world_size)\n            if self.args.global_rank == 0 and (self.iteration % self.args.print_every == 0): \n                pbar.update(self.args.print_every)\n                description = f'mt:{timer_model.release():.1f}s, dt:{timer_data.release():.1f}s, '\n                for key, val in losses.items(): \n                    description += f'{key}:{val.item():.3f}, '\n                    if self.args.tensorboard: \n                        self.writer.add_scalar(key, val.item(), self.iteration)\n                pbar.set_description((description))\n                if self.args.tensorboard: \n                    self.writer.add_image('mask', make_grid(masks), self.iteration)\n                    self.writer.add_image('orig', make_grid((images+1.0)/2.0), self.iteration)\n                    self.writer.add_image('pred', make_grid((pred_img+1.0)/2.0), self.iteration)\n                    self.writer.add_image('comp', make_grid((comp_img+1.0)/2.0), self.iteration)\n                    \n            \n            if self.args.global_rank == 0 and (self.iteration % self.args.save_every) == 0: \n                self.save()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:50:25.044916Z","iopub.execute_input":"2022-02-25T12:50:25.047417Z","iopub.status.idle":"2022-02-25T12:50:25.447593Z","shell.execute_reply.started":"2022-02-25T12:50:25.047368Z","shell.execute_reply":"2022-02-25T12:50:25.44672Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# UTILS FOLDER","metadata":{}},{"cell_type":"markdown","source":"# utils/painter.py","metadata":{}},{"cell_type":"code","source":"import cv2 \nimport sys\n\n\nclass Sketcher:\n    def __init__(self, windowname, dests, colors_func, thick, type):\n        self.prev_pt = None\n        self.windowname = windowname\n        self.dests = dests\n        self.colors_func = colors_func\n        self.dirty = False\n        self.show()\n        self.thick = thick\n        if type == 'bbox':\n            cv2.setMouseCallback(self.windowname, self.on_bbox)\n        else:\n            cv2.setMouseCallback(self.windowname, self.on_mouse)\n\n    def large_thick(self,):  \n        self.thick = min(48, self.thick + 1)\n    \n    def small_thick(self,): \n        self.thick = max(3, self.thick - 1)\n        \n    def show(self):\n        cv2.imshow(self.windowname, self.dests[0])\n\n    def on_mouse(self, event, x, y, flags, param):\n        pt = (x, y)\n        if event == cv2.EVENT_LBUTTONDOWN:\n            self.prev_pt = pt\n        elif event == cv2.EVENT_LBUTTONUP:\n            self.prev_pt = None\n\n        if self.prev_pt and flags & cv2.EVENT_FLAG_LBUTTON:\n            for dst, color in zip(self.dests, self.colors_func()):\n                cv2.line(dst, self.prev_pt, pt, color, self.thick)\n            self.dirty = True\n            self.prev_pt = pt\n            self.show()\n\n    def on_bbox(self, event, x, y, flags, param):\n        pt = (x, y)\n        if event == cv2.EVENT_LBUTTONDOWN:\n            self.prev_pt = pt \n        elif event == cv2.EVENT_LBUTTONUP:\n            for dst, color in zip(self.dests, self.colors_func()):\n                cv2.rectangle(dst, self.prev_pt, pt, color, -1)\n            self.dirty = True\n            self.prev_pt = None\n            self.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:50:28.282467Z","iopub.execute_input":"2022-02-25T12:50:28.282711Z","iopub.status.idle":"2022-02-25T12:50:28.674994Z","shell.execute_reply.started":"2022-02-25T12:50:28.282684Z","shell.execute_reply":"2022-02-25T12:50:28.674183Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# utils/option.py","metadata":{}},{"cell_type":"code","source":"class Namespace:\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n\n#dir_image_arg='../../dataset'\n#dir_mask_arg='../../dataset'\ndir_image_arg='/tmp/ztemp/dataset/image'\ndir_mask_arg='/tmp/ztemp/dataset/mask'\ndata_train_arg='icons'\ndata_test_arg='icons'\nimage_size_arg=256\nmask_type_arg='pconv'\nmodel_arg='aotgan'\nblock_num_arg=8\nrates_arg='1+2+4+8'\ngan_type_arg='smgan'\nseed_arg=2021\nnum_workers_arg=2\nlrg_arg=1e-4\nlrd_arg=1e-4\noptimizer_arg='ADAM'\nbeta1_arg=0.5\nbeta2_arg=0.999\nrec_loss_arg='1*L1+250*Style+0.1*Perceptual'\nadv_weight_arg=0.01\niterations_arg=1e4\nbatch_size_arg=8\nport_arg=22334\nresume_arg='store_true'\nprint_every_arg=10\nsave_every_arg=1e3\nsave_dir_arg='/tmp/ztemp/dataset/experiments'\ntensorboard_arg='store_true'\npre_train_arg='/tmp/ztemp/dataset/pretrain'\noutputs_arg='/tmp/ztemp/dataset/outputs'\nthick_arg=15\npainter_arg='freeform'\n\nargs=Namespace(dir_image=dir_image_arg,\n                dir_mask=dir_mask_arg,\n                data_train=data_train_arg,\n                data_test=data_test_arg,\n                image_size=image_size_arg,\n                mask_type=mask_type_arg,\n                model=model_arg,\n                block_num=block_num_arg,\n                rates=rates_arg,\n                gan_type=gan_type_arg,\n                seed=seed_arg,\n                num_workers=num_workers_arg,\n                lrg=lrg_arg,\n                lrd=lrd_arg,\n                optimizer=optimizer_arg,\n                beta1=beta1_arg,\n                beta2=beta2_arg,\n                rec_loss=rec_loss_arg,\n                adv_weight=adv_weight_arg,\n                iterations=iterations_arg,\n                batch_size=batch_size_arg,\n                port=port_arg,\n                resume=resume_arg,\n                print_every=print_every_arg,\n                save_every=save_every_arg,\n                tensorboard=tensorboard_arg,\n                pre_train=pre_train_arg,\n                outputs=outputs_arg,\n                thick=thick_arg,\n                painter=painter_arg,\n                save_dir=save_dir_arg)\n# ----------------------------------\nargs.iterations = int(args.iterations)\n\nargs.rates = list(map(int, list(args.rates.split('+'))))\n\nlosses = list(args.rec_loss.split('+'))\nargs.rec_loss = {}\nfor l in losses: \n    weight, name = l.split('*')\n    args.rec_loss[name] = float(weight)\nprint(args.rec_loss)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:50:31.729947Z","iopub.execute_input":"2022-02-25T12:50:31.730335Z","iopub.status.idle":"2022-02-25T12:50:31.754374Z","shell.execute_reply.started":"2022-02-25T12:50:31.730288Z","shell.execute_reply":"2022-02-25T12:50:31.753625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MAIN FOLDER","metadata":{}},{"cell_type":"markdown","source":"# demo.py","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\nimport importlib\nimport numpy as np\nfrom glob import glob \n\nimport torch\nfrom torchvision.transforms import ToTensor\n\ndef postprocess(image):\n    image = torch.clamp(image, -1., 1.)\n    image = (image + 1) / 2.0 * 255.0\n    image = image.permute(1, 2, 0)\n    image = image.cpu().numpy().astype(np.uint8)\n    return image\n\n\n\ndef demo(args):\n    # load images \n    img_list = []\n    for ext in ['*.jpg', '*.png']: \n        img_list.extend(glob(os.path.join(args.dir_image, ext)))\n    img_list.sort()\n\n    # Model and version\n    net = importlib.import_module('model.'+args.model)\n    model = net.InpaintGenerator(args)\n    model.load_state_dict(torch.load(args.pre_train, map_location='cpu'))\n    model.eval()\n\n    for fn in img_list:\n        filename = os.path.basename(fn).split('.')[0]\n        orig_img = cv2.resize(cv2.imread(fn, cv2.IMREAD_COLOR), (512, 512))\n        img_tensor = (ToTensor()(orig_img) * 2.0 - 1.0).unsqueeze(0)\n        h, w, c = orig_img.shape\n        mask = np.zeros([h, w, 1], np.uint8)\n        image_copy = orig_img.copy()\n        sketch = Sketcher(\n            'input', [image_copy, mask], lambda: ((255, 255, 255), (255, 255, 255)), args.thick, args.painter)\n\n        while True:\n            ch = cv2.waitKey()\n            if ch == 27:\n                print(\"quit!\")\n                break\n\n            # inpaint by deep model\n            elif ch == ord(' '):\n                print('[**] inpainting ... ')\n                with torch.no_grad():\n                    mask_tensor = (ToTensor()(mask)).unsqueeze(0)\n                    masked_tensor = (img_tensor * (1 - mask_tensor).float()) + mask_tensor\n                    pred_tensor = model(masked_tensor, mask_tensor)\n                    comp_tensor = (pred_tensor * mask_tensor + img_tensor * (1 - mask_tensor))\n\n                    pred_np = postprocess(pred_tensor[0])\n                    masked_np = postprocess(masked_tensor[0])\n                    comp_np = postprocess(comp_tensor[0])\n\n                    cv2.imshow('pred_images', comp_np)\n                    print('inpainting finish!')\n            \n            # reset mask\n            elif ch == ord('r'):\n                img_tensor = (ToTensor()(orig_img) * 2.0 - 1.0).unsqueeze(0)\n                image_copy[:] = orig_img.copy()\n                mask[:] = 0\n                sketch.show()\n                print(\"[**] reset!\")\n\n            # next case\n            elif ch == ord('n'):\n                print('[**] move to next image')\n                cv2.destroyAllWindows()\n                break\n\n            elif ch == ord('k'): \n                print('[**] apply existing processing to images, and keep editing!')\n                img_tensor = comp_tensor\n                image_copy[:] = comp_np.copy()\n                mask[:] = 0\n                sketch.show()\n                print(\"reset!\")\n            \n            elif ch == ord('+'): \n                sketch.large_thick()\n\n            elif ch == ord('-'): \n                sketch.small_thick()\n            \n            # save results\n            if ch == ord('s'):\n                cv2.imwrite(os.path.join(args.outputs, f'{filename}_masked.png'), masked_np)\n                cv2.imwrite(os.path.join(args.outputs, f'{filename}_pred.png'), pred_np)\n                cv2.imwrite(os.path.join(args.outputs, f'{filename}_comp.png'), comp_np)\n                cv2.imwrite(os.path.join(args.outputs, f'{filename}_mask.png'), mask)\n\n                print('[**] save successfully!')\n        cv2.destroyAllWindows()\n\n        if ch == 27:\n            break\n\n\nif __name__ == '__main__':\n    demo(args)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:09:29.469967Z","iopub.status.idle":"2022-02-09T08:09:29.47103Z","shell.execute_reply.started":"2022-02-09T08:09:29.470615Z","shell.execute_reply":"2022-02-09T08:09:29.470666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# eval.py","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\nfrom glob import glob\nfrom PIL import Image\nfrom multiprocessing import Pool\n\nclass Namespace:\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\nreal_dir_arg='' #required\nfake_dir_arg='' #required\nmetric_arg=\"+\"\nargs=Namespace(real_dir=real_dir_arg,\n                fake_dir=fake_dir_arg,\n                metric=metric_arg)\n\ndef read_img(name_pair): \n    rname, fname = name_pair\n    rimg = Image.open(rname)\n    fimg = Image.open(fname)\n    return np.array(rimg), np.array(fimg)\n\n\ndef main(num_worker=8):\n\n    real_names = sorted(list(glob(f'{args.real_dir}/*.png')))\n    fake_names = sorted(list(glob(f'{args.fake_dir}/*.png')))\n    print(f'real images: {len(real_names)}, fake images: {len(fake_names)}')\n    real_images = []\n    fake_images = []\n    pool = Pool(num_worker)\n    for rimg, fimg in tqdm(pool.imap_unordered(read_img, zip(real_names, fake_names)), total=len(real_names), desc='loading images'):\n        real_images.append(rimg)\n        fake_images.append(fimg)\n\n\n    # metrics prepare for image assesments\n    metrics = {met: getattr(module_metric, met) for met in args.metric}\n    evaluation_scores = {key: 0 for key,val in metrics.items()}\n    for key, val in metrics.items():\n        evaluation_scores[key] = val(real_images, fake_images, num_worker=num_worker)\n    print(' '.join(['{}: {:6f},'.format(key, val) for key,val in evaluation_scores.items()]))\n  \n  \n\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:09:29.473016Z","iopub.status.idle":"2022-02-09T08:09:29.473836Z","shell.execute_reply.started":"2022-02-09T08:09:29.473564Z","shell.execute_reply":"2022-02-09T08:09:29.473593Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# test.py","metadata":{}},{"cell_type":"code","source":"import os\nimport argparse\nimport importlib\nimport numpy as np\nfrom PIL import Image\nfrom glob import glob\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.transforms import ToTensor \n\ndef postprocess(image):\n    image = torch.clamp(image, -1., 1.)\n    image = (image + 1) / 2.0 * 255.0\n    image = image.permute(1, 2, 0)\n    image = image.cpu().numpy().astype(np.uint8)\n    return Image.fromarray(image)\n\n\ndef main_worker(args, use_gpu=True): \n\n    device = torch.device('cuda') if use_gpu else torch.device('cpu')\n    \n    # Model and version\n    net = importlib.import_module('model.'+args.model)\n    model = net.InpaintGenerator(args).cuda()\n    model.load_state_dict(torch.load(args.pre_train, map_location='cuda'))\n    model.eval()\n\n    # prepare dataset\n    image_paths = []\n    for ext in ['.jpg', '.png']: \n        image_paths.extend(glob(os.path.join(args.dir_image, '*'+ext)))\n    image_paths.sort()\n    mask_paths = sorted(glob(os.path.join(args.dir_mask, '*.png')))\n    os.makedirs(args.outputs, exist_ok=True)\n    \n    # iteration through datasets\n    for ipath, mpath in zip(image_paths, mask_paths): \n        image = ToTensor()(Image.open(ipath).convert('RGB'))\n        image = (image * 2.0 - 1.0).unsqueeze(0)\n        mask = ToTensor()(Image.open(mpath).convert('L'))\n        mask = mask.unsqueeze(0)\n        image, mask = image.cuda(), mask.cuda()\n        image_masked = image * (1 - mask.float()) + mask\n        \n        with torch.no_grad():\n            pred_img = model(image_masked, mask)\n\n        comp_imgs = (1 - mask) * image + mask * pred_img\n        image_name = os.path.basename(ipath).split('.')[0]\n        postprocess(image_masked[0]).save(os.path.join(args.outputs, f'{image_name}_masked.png'))\n        postprocess(pred_img[0]).save(os.path.join(args.outputs, f'{image_name}_pred.png'))\n        postprocess(comp_imgs[0]).save(os.path.join(args.outputs, f'{image_name}_comp.png'))\n        print(f'saving to {os.path.join(args.outputs, image_name)}')\n\n\nif __name__ == '__main__':\n    main_worker(args)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-09T08:09:29.475581Z","iopub.status.idle":"2022-02-09T08:09:29.482738Z","shell.execute_reply.started":"2022-02-09T08:09:29.482334Z","shell.execute_reply":"2022-02-09T08:09:29.482403Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# train.py","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\n#from dataloader import *\nimport torch.multiprocessing as mp\n\ndef main_worker(id, ngpus_per_node, args):\n    args.local_rank = args.global_rank = id\n    if args.distributed:\n        torch.cuda.set_device(args.local_rank)\n        print(f'using GPU {args.world_size}-{args.global_rank} for training')\n        torch.distributed.init_process_group(\n            backend='nccl', init_method=args.init_method,\n            world_size=args.world_size, rank=args.global_rank,\n            group_name='mtorch')\n\n    args.save_dir = os.path.join(\n        args.save_dir, f'{args.model}_{args.data_train}_{args.mask_type}{args.image_size}')\n        \n    if (not args.distributed) or args.global_rank == 0:\n        os.makedirs(args.save_dir, exist_ok=True)\n        with open(os.path.join(args.save_dir, 'config.txt'), 'a') as f:\n            for key, val in vars(args).items(): \n                f.write(f'{key}: {val}\\n')\n        print(f'[**] create folder {args.save_dir}')\n\n    trainer = Trainer(args)\n    trainer.train()\n\n\nif __name__ == \"__main__\":\n    \n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.enabled = True\n    torch.backends.cudnn.benchmark = True\n\n    # setup distributed parallel training environments\n    ngpus_per_node = torch.cuda.device_count()\n    if ngpus_per_node > 1:\n        args.world_size = ngpus_per_node\n        args.init_method = f'tcp://127.0.0.1:{args.port}'\n        args.distributed = True\n        mp.spawn(main_worker, nprocs=ngpus_per_node,\n                 args=(ngpus_per_node, args))\n    else:\n        args.world_size = 1\n        args.distributed = False \n        main_worker(0, 1, args)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T12:50:40.081518Z","iopub.execute_input":"2022-02-25T12:50:40.08177Z","iopub.status.idle":"2022-02-25T15:54:36.387709Z","shell.execute_reply.started":"2022-02-25T12:50:40.08174Z","shell.execute_reply":"2022-02-25T15:54:36.386665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# COPY MODEL TRAINING RESULTS TO OUTPUT","metadata":{}},{"cell_type":"code","source":"!cp /tmp/ztemp/dataset/experiments/D0000000.pt /kaggle/working\n!cp /tmp/ztemp/dataset/experiments/G0000000.pt /kaggle/working\n!cp /tmp/ztemp/dataset/experiments/O0000000.pt /kaggle/working\n!cp /tmp/ztemp/dataset/experiments/aotgan_icons_pconv256/config.txt /kaggle/working\n!cp /tmp/ztemp/dataset/experiments/aotgan_icons_pconv256/D0010000.pt /kaggle/working\n!cp /tmp/ztemp/dataset/experiments/aotgan_icons_pconv256/O0010000.pt /kaggle/working\n!cp /tmp/ztemp/dataset/experiments/aotgan_icons_pconv256/G0010000.pt /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2022-02-25T15:55:12.591916Z","iopub.execute_input":"2022-02-25T15:55:12.592501Z","iopub.status.idle":"2022-02-25T15:57:43.273619Z","shell.execute_reply.started":"2022-02-25T15:55:12.592464Z","shell.execute_reply":"2022-02-25T15:57:43.272469Z"},"trusted":true},"execution_count":null,"outputs":[]}]}